{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPKwqqmwCJQ3CMAqALKuYr8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/surajgajul/Friends-GPT/blob/main/Friends.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJWba63JrhWJ",
        "outputId": "37e38787-f224-4b8e-c091-2f0a3d3851ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/Final_transcript.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "id": "_1yTYS65sT9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"length of dataset in characters: \", len(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ykj-YcoAsh0E",
        "outputId": "10965d99-ad2e-4db7-d435-43989fc1860a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters:  4621467\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JAFaw--0siw5",
        "outputId": "8b528694-8c92-4917-9542-17812efa2393"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "THE ONE WHERE MONICA GETS A NEW ROOMATE (THE PILOT-THE UNCUT VERSION)\n",
            "[Scene: Central Perk, Chandler, Joey, Phoebe, and Monica are there.]\n",
            "Monica: There's nothing to tell! He's just some guy I work with!\n",
            "Joey: C'mon, you're going out with the guy! There's gotta be something wrong with him!\n",
            "Chandler: All right Joey, be nice. So does he have a hump? A hump and a hairpiece?\n",
            "Phoebe: Wait, does he eat chalk?\n",
            "(They all stare, bemused.)\n",
            "Phoebe: Just, 'cause, I don't want her to go through what I went through with Carl- oh!\n",
            "Monica: Okay, everybody relax. This is not even a date. It's just two people going out to dinner and- not having sex.\n",
            "Chandler: Sounds like a date to me.\n",
            "[Time Lapse]\n",
            "Chandler: Alright, so I'm back in high school, I'm standing in the middle of the cafeteria, and I realize I am totally naked.\n",
            "All: Oh, yeah. Had that dream.\n",
            "Chandler: Then I look down, and I realize there's a phone... there.\n",
            "Joey: Instead of...?\n",
            "Chandler: That's right.\n",
            "Joey: Never had that dream.\n",
            "Phoebe: No.\n",
            "C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1CEDvYJhsk5w",
        "outputId": "87ac76c1-a255-4f55-f3eb-3f5aac17e965"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !\"#$%&'()*+,-./0123456789:;<=>?ABCDEFGHIJKLMNOPQRSTUVWXYZ[]^_`abcdefghijklmnopqrstuvwxyz{|}\n",
            "93\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Try out tiktoken for smaller encoding and larger vocab**"
      ],
      "metadata": {
        "id": "NzRlbXWyu2HJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "print(encode(\"hii there\"))\n",
        "print(decode(encode(\"hii there\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yc--vCJes5A0",
        "outputId": "9b13ff3a-0ced-4dea-ed1a-a87f94435bf8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[71, 72, 72, 1, 83, 71, 68, 81, 68]\n",
            "hii there\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch # we use PyTorch: https://pytorch.org\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6XETcTGRuSS1",
        "outputId": "5bd78fa0-f55f-4270-8e49-1b869ca6c9a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4621467]) torch.int64\n",
            "tensor([52, 40, 37,  1, 47, 46, 37,  1, 55, 40, 37, 50, 37,  1, 45, 47, 46, 41,\n",
            "        35, 33,  1, 39, 37, 52, 51,  1, 33,  1, 46, 37, 55,  1, 50, 47, 47, 45,\n",
            "        33, 52, 37,  1,  9, 52, 40, 37,  1, 48, 41, 44, 47, 52, 14, 52, 40, 37,\n",
            "         1, 53, 46, 35, 53, 52,  1, 54, 37, 50, 51, 41, 47, 46, 10,  0, 59, 51,\n",
            "        66, 68, 77, 68, 27,  1, 35, 68, 77, 83, 81, 64, 75,  1, 48, 68, 81, 74,\n",
            "        13,  1, 35, 71, 64, 77, 67, 75, 68, 81, 13,  1, 42, 78, 68, 88, 13,  1,\n",
            "        48, 71, 78, 68, 65, 68, 13,  1, 64, 77, 67,  1, 45, 78, 77, 72, 66, 64,\n",
            "         1, 64, 81, 68,  1, 83, 71, 68, 81, 68, 15, 60,  0, 45, 78, 77, 72, 66,\n",
            "        64, 27,  1, 52, 71, 68, 81, 68,  8, 82,  1, 77, 78, 83, 71, 72, 77, 70,\n",
            "         1, 83, 78,  1, 83, 68, 75, 75,  2,  1, 40, 68,  8, 82,  1, 73, 84, 82,\n",
            "        83,  1, 82, 78, 76, 68,  1, 70, 84, 88,  1, 41,  1, 86, 78, 81, 74,  1,\n",
            "        86, 72, 83, 71,  2,  0, 42, 78, 68, 88, 27,  1, 35,  8, 76, 78, 77, 13,\n",
            "         1, 88, 78, 84,  8, 81, 68,  1, 70, 78, 72, 77, 70,  1, 78, 84, 83,  1,\n",
            "        86, 72, 83, 71,  1, 83, 71, 68,  1, 70, 84, 88,  2,  1, 52, 71, 68, 81,\n",
            "        68,  8, 82,  1, 70, 78, 83, 83, 64,  1, 65, 68,  1, 82, 78, 76, 68, 83,\n",
            "        71, 72, 77, 70,  1, 86, 81, 78, 77, 70,  1, 86, 72, 83, 71,  1, 71, 72,\n",
            "        76,  2,  0, 35, 71, 64, 77, 67, 75, 68, 81, 27,  1, 33, 75, 75,  1, 81,\n",
            "        72, 70, 71, 83,  1, 42, 78, 68, 88, 13,  1, 65, 68,  1, 77, 72, 66, 68,\n",
            "        15,  1, 51, 78,  1, 67, 78, 68, 82,  1, 71, 68,  1, 71, 64, 85, 68,  1,\n",
            "        64,  1, 71, 84, 76, 79, 32,  1, 33,  1, 71, 84, 76, 79,  1, 64, 77, 67,\n",
            "         1, 64,  1, 71, 64, 72, 81, 79, 72, 68, 66, 68, 32,  0, 48, 71, 78, 68,\n",
            "        65, 68, 27,  1, 55, 64, 72, 83, 13,  1, 67, 78, 68, 82,  1, 71, 68,  1,\n",
            "        68, 64, 83,  1, 66, 71, 64, 75, 74, 32,  0,  9, 52, 71, 68, 88,  1, 64,\n",
            "        75, 75,  1, 82, 83, 64, 81, 68, 13,  1, 65, 68, 76, 84, 82, 68, 67, 15,\n",
            "        10,  0, 48, 71, 78, 68, 65, 68, 27,  1, 42, 84, 82, 83, 13,  1,  8, 66,\n",
            "        64, 84, 82, 68, 13,  1, 41,  1, 67, 78, 77,  8, 83,  1, 86, 64, 77, 83,\n",
            "         1, 71, 68, 81,  1, 83, 78,  1, 70, 78,  1, 83, 71, 81, 78, 84, 70, 71,\n",
            "         1, 86, 71, 64, 83,  1, 41,  1, 86, 68, 77, 83,  1, 83, 71, 81, 78, 84,\n",
            "        70, 71,  1, 86, 72, 83, 71,  1, 35, 64, 81, 75, 14,  1, 78, 71,  2,  0,\n",
            "        45, 78, 77, 72, 66, 64, 27,  1, 47, 74, 64, 88, 13,  1, 68, 85, 68, 81,\n",
            "        88, 65, 78, 67, 88,  1, 81, 68, 75, 64, 87, 15,  1, 52, 71, 72, 82,  1,\n",
            "        72, 82,  1, 77, 78, 83,  1, 68, 85, 68, 77,  1, 64,  1, 67, 64, 83, 68,\n",
            "        15,  1, 41, 83,  8, 82,  1, 73, 84, 82, 83,  1, 83, 86, 78,  1, 79, 68,\n",
            "        78, 79, 75, 68,  1, 70, 78, 72, 77, 70,  1, 78, 84, 83,  1, 83, 78,  1,\n",
            "        67, 72, 77, 77, 68, 81,  1, 64, 77, 67, 14,  1, 77, 78, 83,  1, 71, 64,\n",
            "        85, 72, 77, 70,  1, 82, 68, 87, 15,  0, 35, 71, 64, 77, 67, 75, 68, 81,\n",
            "        27,  1, 51, 78, 84, 77, 67, 82,  1, 75, 72, 74, 68,  1, 64,  1, 67, 64,\n",
            "        83, 68,  1, 83, 78,  1, 76, 68, 15,  0, 59, 52, 72, 76, 68,  1, 44, 64,\n",
            "        79, 82, 68, 60,  0, 35, 71, 64, 77, 67, 75, 68, 81, 27,  1, 33, 75, 81,\n",
            "        72, 70, 71, 83, 13,  1, 82, 78,  1, 41,  8, 76,  1, 65, 64, 66, 74,  1,\n",
            "        72, 77,  1, 71, 72, 70, 71,  1, 82, 66, 71, 78, 78, 75, 13,  1, 41,  8,\n",
            "        76,  1, 82, 83, 64, 77, 67, 72, 77, 70,  1, 72, 77,  1, 83, 71, 68,  1,\n",
            "        76, 72, 67, 67, 75, 68,  1, 78, 69,  1, 83, 71, 68,  1, 66, 64, 69, 68,\n",
            "        83, 68, 81, 72, 64, 13,  1, 64, 77, 67,  1, 41,  1, 81, 68, 64, 75, 72,\n",
            "        89, 68,  1, 41,  1, 64, 76,  1, 83, 78, 83, 64, 75, 75, 88,  1, 77, 64,\n",
            "        74, 68, 67, 15,  0, 33, 75, 75, 27,  1, 47, 71, 13,  1, 88, 68, 64, 71,\n",
            "        15,  1, 40, 64, 67,  1, 83, 71, 64, 83,  1, 67, 81, 68, 64, 76, 15,  0,\n",
            "        35, 71, 64, 77, 67, 75, 68, 81, 27,  1, 52, 71, 68, 77,  1, 41,  1, 75,\n",
            "        78, 78, 74,  1, 67, 78, 86, 77, 13,  1, 64, 77, 67,  1, 41,  1, 81, 68,\n",
            "        64, 75, 72, 89, 68,  1, 83, 71, 68, 81, 68,  8, 82,  1, 64,  1, 79, 71,\n",
            "        78, 77, 68, 15, 15, 15,  1, 83, 71, 68, 81, 68, 15,  0, 42, 78, 68, 88,\n",
            "        27,  1, 41, 77, 82, 83, 68, 64, 67,  1, 78, 69, 15, 15, 15, 32,  0, 35,\n",
            "        71, 64, 77, 67, 75, 68, 81, 27,  1, 52, 71, 64, 83,  8, 82,  1, 81, 72,\n",
            "        70, 71, 83, 15,  0, 42, 78, 68, 88, 27,  1, 46, 68, 85, 68, 81,  1, 71,\n",
            "        64, 67,  1, 83, 71, 64, 83,  1, 67, 81, 68, 64, 76, 15,  0, 48, 71, 78,\n",
            "        68, 65, 68, 27,  1, 46, 78, 15,  0, 35])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "Sd0bCfQ1vYL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 8\n",
        "train_data[:block_size+1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SGP5BkQ-wxc1",
        "outputId": "149d9da9-3d7d-4108-fe91-6b8592ebac4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([52, 40, 37,  1, 47, 46, 37,  1, 55])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size+1]\n",
        "for t in range(block_size):\n",
        "    context = x[:t+1]\n",
        "    target = y[t]\n",
        "    print(f\"when input is {context} the target: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "geG-67CWxGhU",
        "outputId": "40626b30-397c-4598-83e9-b1d6193210e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when input is tensor([52]) the target: 40\n",
            "when input is tensor([52, 40]) the target: 37\n",
            "when input is tensor([52, 40, 37]) the target: 1\n",
            "when input is tensor([52, 40, 37,  1]) the target: 47\n",
            "when input is tensor([52, 40, 37,  1, 47]) the target: 46\n",
            "when input is tensor([52, 40, 37,  1, 47, 46]) the target: 37\n",
            "when input is tensor([52, 40, 37,  1, 47, 46, 37]) the target: 1\n",
            "when input is tensor([52, 40, 37,  1, 47, 46, 37,  1]) the target: 55\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "batch_size = 4 # how many independent sequences will we process in parallel?\n",
        "block_size = 8 # what is the maximum context length for predictions?\n",
        "\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    return x, y\n",
        "\n",
        "xb, yb = get_batch('train')\n",
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('targets:')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "print('----')\n",
        "\n",
        "for b in range(batch_size): # batch dimension\n",
        "    for t in range(block_size): # time dimension\n",
        "        context = xb[b, :t+1]\n",
        "        target = yb[b,t]\n",
        "        print(f\"when input is {context.tolist()} the target: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2Pj1NxHxl7Q",
        "outputId": "a3690013-ab7a-4fe0-8e70-aa32ddadfce8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs:\n",
            "torch.Size([4, 8])\n",
            "tensor([[78, 74, 13,  1, 41,  1, 83, 71],\n",
            "        [47, 71,  1, 77, 78,  0, 48, 71],\n",
            "        [68, 67,  1, 84, 79,  1, 71, 72],\n",
            "        [82, 71, 68, 82,  1, 83, 71, 68]])\n",
            "targets:\n",
            "torch.Size([4, 8])\n",
            "tensor([[74, 13,  1, 41,  1, 83, 71, 72],\n",
            "        [71,  1, 77, 78,  0, 48, 71, 78],\n",
            "        [67,  1, 84, 79,  1, 71, 72, 82],\n",
            "        [71, 68, 82,  1, 83, 71, 68,  1]])\n",
            "----\n",
            "when input is [78] the target: 74\n",
            "when input is [78, 74] the target: 13\n",
            "when input is [78, 74, 13] the target: 1\n",
            "when input is [78, 74, 13, 1] the target: 41\n",
            "when input is [78, 74, 13, 1, 41] the target: 1\n",
            "when input is [78, 74, 13, 1, 41, 1] the target: 83\n",
            "when input is [78, 74, 13, 1, 41, 1, 83] the target: 71\n",
            "when input is [78, 74, 13, 1, 41, 1, 83, 71] the target: 72\n",
            "when input is [47] the target: 71\n",
            "when input is [47, 71] the target: 1\n",
            "when input is [47, 71, 1] the target: 77\n",
            "when input is [47, 71, 1, 77] the target: 78\n",
            "when input is [47, 71, 1, 77, 78] the target: 0\n",
            "when input is [47, 71, 1, 77, 78, 0] the target: 48\n",
            "when input is [47, 71, 1, 77, 78, 0, 48] the target: 71\n",
            "when input is [47, 71, 1, 77, 78, 0, 48, 71] the target: 78\n",
            "when input is [68] the target: 67\n",
            "when input is [68, 67] the target: 1\n",
            "when input is [68, 67, 1] the target: 84\n",
            "when input is [68, 67, 1, 84] the target: 79\n",
            "when input is [68, 67, 1, 84, 79] the target: 1\n",
            "when input is [68, 67, 1, 84, 79, 1] the target: 71\n",
            "when input is [68, 67, 1, 84, 79, 1, 71] the target: 72\n",
            "when input is [68, 67, 1, 84, 79, 1, 71, 72] the target: 82\n",
            "when input is [82] the target: 71\n",
            "when input is [82, 71] the target: 68\n",
            "when input is [82, 71, 68] the target: 82\n",
            "when input is [82, 71, 68, 82] the target: 1\n",
            "when input is [82, 71, 68, 82, 1] the target: 83\n",
            "when input is [82, 71, 68, 82, 1, 83] the target: 71\n",
            "when input is [82, 71, 68, 82, 1, 83, 71] the target: 68\n",
            "when input is [82, 71, 68, 82, 1, 83, 71, 68] the target: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(xb) # our input to the transformer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DKiOoNE0yrk4",
        "outputId": "c622b444-623a-465a-de20-dd8f4148c1a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[78, 74, 13,  1, 41,  1, 83, 71],\n",
            "        [47, 71,  1, 77, 78,  0, 48, 71],\n",
            "        [68, 67,  1, 84, 79,  1, 71, 72],\n",
            "        [82, 71, 68, 82,  1, 83, 71, 68]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "m = BigramLanguageModel(vocab_size)\n",
        "logits, loss = m(xb, yb)\n",
        "print(logits.shape)\n",
        "print(loss)\n",
        "\n",
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9aZQEpzA1Gkx",
        "outputId": "8996b4c4-0324-4ce2-87fa-683b7552ec15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 93])\n",
            "tensor(5.0291, grad_fn=<NllLossBackward0>)\n",
            "\n",
            "qAk'|WCHrrI%)Gs,f&:xU'l9BktiWkKwE|J:4N(fW/Q+:9\n",
            "1FPfIppvkj?\n",
            "qOhe1anRB\"<{cjdIGVukm -s5LK_ait(e5g[Ofnt7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer=torch.optim.AdamW(m.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "ZZH0TxqQ5S7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "for steps in range(10000): # increase number of steps for good results...\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = m(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "print(loss.item())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43IQ5mlShGBT",
        "outputId": "f1100d37-214e-426b-8a4a-22c61404309d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.404139757156372\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p91eXPRChj8W",
        "outputId": "cdb22b40-ff07-4b09-82b0-46e6b955725d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[STherts. Pete arsTheat ho whausshit, Suf w, Risthingly? s.'s acan a: terRo se Gowat'6HeybWhonothanth h whe ha at an!\n",
            "Moure pot. is) bursh, I fon (funist Pel: bu s ban (Phinthealeay I'd Soper beve2, dokinte, Mo d rovjouk s beve-ore thechich y (Jouy abuh (ENaront toan: avjug: cl, atey N(r songis'm inevind I he?\n",
            "(I Noutha hOkave: awan't*q6pll: BVEAl win iagatho m Ped a ging I as: f fell ave N+n myofo le m, Hemind t thiroo-Cran'lelebeadotsakis thowar)\n",
            "Roenaplithan athidllelelex. shas m irigo. butha\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "B, T, C=4,8,2\n",
        "x=torch.randn(B, T, C)\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jpfNhJLGmB0L",
        "outputId": "60b635e2-13c9-41bf-9f1f-14b096e11974"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xbow = torch.zeros((B,T,C))\n",
        "for b in range(B):\n",
        "    for t in range(T):\n",
        "        xprev = x[b,:t+1] # (t,C)\n",
        "        xbow[b,t] = torch.mean(xprev, 0)"
      ],
      "metadata": {
        "id": "jQctrI5jnK3y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iIghPZk4njnV",
        "outputId": "178a14ba-c16c-4bf6-a877-8f712cbacbbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1.9269,  1.4873],\n",
              "        [ 0.9007, -2.1055],\n",
              "        [ 0.6784, -1.2345],\n",
              "        [-0.0431, -1.6047],\n",
              "        [-0.7521,  1.6487],\n",
              "        [-0.3925, -1.4036],\n",
              "        [-0.7279, -0.5594],\n",
              "        [-0.7688,  0.7624]])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xbow[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2__UTed7n6PM",
        "outputId": "6a7c6d3d-9b1b-4d3c-be83-cfd30ddd653b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1.9269,  1.4873],\n",
              "        [ 1.4138, -0.3091],\n",
              "        [ 1.1687, -0.6176],\n",
              "        [ 0.8657, -0.8644],\n",
              "        [ 0.5422, -0.3617],\n",
              "        [ 0.3864, -0.5354],\n",
              "        [ 0.2272, -0.5388],\n",
              "        [ 0.1027, -0.3762]])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n",
        "torch.manual_seed(42)\n",
        "a = torch.tril(torch.ones(3, 3))\n",
        "a = a / torch.sum(a, 1, keepdim=True)\n",
        "b = torch.randint(0,10,(3,2)).float()\n",
        "c = a @ b\n",
        "print('a=')\n",
        "print(a)\n",
        "print('--')\n",
        "print('b=')\n",
        "print(b)\n",
        "print('--')\n",
        "print('c=')\n",
        "print(c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HCc7Iko5n9xs",
        "outputId": "29c9c920-bdb8-4bd7-dbb8-6162c1f28ca9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a=\n",
            "tensor([[1.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333]])\n",
            "--\n",
            "b=\n",
            "tensor([[2., 7.],\n",
            "        [6., 4.],\n",
            "        [6., 5.]])\n",
            "--\n",
            "c=\n",
            "tensor([[2.0000, 7.0000],\n",
            "        [4.0000, 5.5000],\n",
            "        [4.6667, 5.3333]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wei = torch.tril(torch.ones(T, T))\n",
        "wei = wei / wei.sum(1, keepdim=True)\n",
        "wei"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qsy5GA28pYMx",
        "outputId": "100fe8e9-e6d7-43f9-cb54-ba36fd521fe3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
              "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
              "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wei = torch.tril(torch.ones(T, T))\n",
        "wei = wei / wei.sum(1, keepdim=True)\n",
        "xbow2 = wei @ x # (B, T, T) @ (B, T, C) ----> (B, T, C)\n",
        "torch.allclose(xbow, xbow2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hk7oKrbrq25U",
        "outputId": "5acb021c-10ac-43fb-8f93-22b1f5a736a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tril = torch.tril(torch.ones(T, T))\n",
        "wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "xbow3 = wei @ x\n",
        "torch.allclose(xbow, xbow3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pcxoN3LLrrIc",
        "outputId": "8624511a-7eb2-4422-a765-dd0f8b8bd7c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# version 4: self-attention!\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,32 # batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "\n",
        "# let's see a single Head perform self-attention\n",
        "head_size = 16\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "value = nn.Linear(C, head_size, bias=False)\n",
        "k = key(x)   # (B, T, 16)\n",
        "q = query(x) # (B, T, 16)\n",
        "wei =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
        "\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "#wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "\n",
        "v = value(x)\n",
        "out = wei @ v\n",
        "#out = wei @ x\n",
        "\n",
        "out.shape"
      ],
      "metadata": {
        "id": "WQbKnHWEsESy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af317957-e709-4da3-cb48-95fda7d4540e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wei"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7jTtRZ0Uw48i",
        "outputId": "81f3b2c1-4a1b-44f8-d64b-181d00fd41f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
              "         [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
              "         [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
              "\n",
              "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.1687, 0.8313, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.2477, 0.0514, 0.7008, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.4410, 0.0957, 0.3747, 0.0887, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0069, 0.0456, 0.0300, 0.7748, 0.1427, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0660, 0.0892, 0.0413, 0.6316, 0.1649, 0.0069, 0.0000, 0.0000],\n",
              "         [0.0396, 0.2288, 0.0090, 0.2000, 0.2061, 0.1949, 0.1217, 0.0000],\n",
              "         [0.3650, 0.0474, 0.0767, 0.0293, 0.3084, 0.0784, 0.0455, 0.0493]],\n",
              "\n",
              "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.4820, 0.5180, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.1705, 0.4550, 0.3745, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0074, 0.7444, 0.0477, 0.2005, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.8359, 0.0416, 0.0525, 0.0580, 0.0119, 0.0000, 0.0000, 0.0000],\n",
              "         [0.1195, 0.2061, 0.1019, 0.1153, 0.1814, 0.2758, 0.0000, 0.0000],\n",
              "         [0.0065, 0.0589, 0.0372, 0.3063, 0.1325, 0.3209, 0.1378, 0.0000],\n",
              "         [0.1416, 0.1519, 0.0384, 0.1643, 0.1207, 0.1254, 0.0169, 0.2408]],\n",
              "\n",
              "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.6369, 0.3631, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.2586, 0.7376, 0.0038, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.4692, 0.3440, 0.1237, 0.0631, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.1865, 0.4680, 0.0353, 0.1854, 0.1248, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0828, 0.7479, 0.0017, 0.0735, 0.0712, 0.0228, 0.0000, 0.0000],\n",
              "         [0.0522, 0.0517, 0.0961, 0.0375, 0.1024, 0.5730, 0.0872, 0.0000],\n",
              "         [0.0306, 0.2728, 0.0333, 0.1409, 0.1414, 0.0582, 0.0825, 0.2402]]],\n",
              "       grad_fn=<SoftmaxBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 16 # how many independent sequences will we process in parallel?\n",
        "block_size = 32 # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.0\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "with open('/content/drive/MyDrive/Final_transcript.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "By0hMZJiw5f1",
        "outputId": "36dd502e-757d-4825-e36a-3d4fa1df68b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.213341 M parameters\n",
            "step 0: train loss 4.7622, val loss 4.7591\n",
            "step 100: train loss 2.6860, val loss 2.6921\n",
            "step 200: train loss 2.4800, val loss 2.4860\n",
            "step 300: train loss 2.3370, val loss 2.3524\n",
            "step 400: train loss 2.2138, val loss 2.2337\n",
            "step 500: train loss 2.1151, val loss 2.1479\n",
            "step 600: train loss 2.0616, val loss 2.0836\n",
            "step 700: train loss 2.0017, val loss 2.0280\n",
            "step 800: train loss 1.9614, val loss 1.9831\n",
            "step 900: train loss 1.9176, val loss 1.9375\n",
            "step 1000: train loss 1.8811, val loss 1.9082\n",
            "step 1100: train loss 1.8359, val loss 1.8801\n",
            "step 1200: train loss 1.8391, val loss 1.8747\n",
            "step 1300: train loss 1.8042, val loss 1.8503\n",
            "step 1400: train loss 1.7910, val loss 1.8250\n",
            "step 1500: train loss 1.7686, val loss 1.7792\n",
            "step 1600: train loss 1.7562, val loss 1.7905\n",
            "step 1700: train loss 1.7393, val loss 1.7844\n",
            "step 1800: train loss 1.7118, val loss 1.7581\n",
            "step 1900: train loss 1.7168, val loss 1.7230\n",
            "step 2000: train loss 1.6994, val loss 1.7363\n",
            "step 2100: train loss 1.6912, val loss 1.7255\n",
            "step 2200: train loss 1.6819, val loss 1.7196\n",
            "step 2300: train loss 1.6735, val loss 1.7050\n",
            "step 2400: train loss 1.6692, val loss 1.6924\n",
            "step 2500: train loss 1.6567, val loss 1.6795\n",
            "step 2600: train loss 1.6471, val loss 1.6896\n",
            "step 2700: train loss 1.6386, val loss 1.6782\n",
            "step 2800: train loss 1.6382, val loss 1.6728\n",
            "step 2900: train loss 1.6167, val loss 1.6492\n",
            "step 3000: train loss 1.6191, val loss 1.6725\n",
            "step 3100: train loss 1.6110, val loss 1.6579\n",
            "step 3200: train loss 1.5958, val loss 1.6355\n",
            "step 3300: train loss 1.6005, val loss 1.6433\n",
            "step 3400: train loss 1.5773, val loss 1.6135\n",
            "step 3500: train loss 1.5868, val loss 1.6228\n",
            "step 3600: train loss 1.5803, val loss 1.6038\n",
            "step 3700: train loss 1.5634, val loss 1.6093\n",
            "step 3800: train loss 1.5589, val loss 1.6109\n",
            "step 3900: train loss 1.5687, val loss 1.5987\n",
            "step 4000: train loss 1.5572, val loss 1.6171\n",
            "step 4100: train loss 1.5622, val loss 1.5967\n",
            "step 4200: train loss 1.5528, val loss 1.5823\n",
            "step 4300: train loss 1.5440, val loss 1.5874\n",
            "step 4400: train loss 1.5541, val loss 1.5875\n",
            "step 4500: train loss 1.5289, val loss 1.5688\n",
            "step 4600: train loss 1.5422, val loss 1.5845\n",
            "step 4700: train loss 1.5307, val loss 1.5639\n",
            "step 4800: train loss 1.5295, val loss 1.5639\n",
            "step 4900: train loss 1.5297, val loss 1.5704\n",
            "step 4999: train loss 1.5229, val loss 1.5635\n",
            "\n",
            "Phoebe: (entering sder) Hi.\n",
            "Rachel: (starts every) Thats guys?\n",
            "Joey: Bone quicelier dame youre!\n",
            "Ross: Ross hen were weara. Seet you my my deary!! Bye hoo!\n",
            "Chandler: Rembed you going out 2 goody-uh, Really!\n",
            "Ross: (on, Rachely gounsen. I thought I get the fallage! (The psets him shouldicalious lepten.\n",
            "Rachel: Dince.\n",
            "ROS(Rachel. (wong of the last promens, and My God says.\n",
            "Chandler: (the fuc) So the chichel: I gotta in belone. (He foids hrops of the lonner lappocens ugelfully presedle of and repaimen in gonna haven's fivulinured.\n",
            "Phoebe: She noted my let it the car. What?\n",
            "Phoebe: I do, hey, he need the rasaination hay one glight won't if it!\n",
            "Joey: Okay?! And honey!\n",
            "Phoebe: Well, yknow. You schuncelly say the down. .. guess! This going it lages a pupslauge Hed lives and y'know what! Why you seep, cary that your supant.\n",
            "Cerciality: No! (vellering to hake carourething for Chandler Try.] ( Molder.) Shes ha wave.) I mean \"Yights, Ross Quns so harre just ccecosed Joey Eally, Pheebs! Why make wy it (looks uns) \n",
            "Murnica: No, I know. I so I do nead while slooking.)\n",
            "CHACH: Yes, and the hhome.\n",
            "Phoebe: (looks, Monicas for my jols too head. (Walking.)\n",
            "[Scened ha4 youre cap been.) Did! Det you?\n",
            "Mr. (She lets him pases left good.)\n",
            "MONIf Frider: Thois shlan like of and. (dang and goes in Pauls Ben.)\n",
            "Rachel: Y'know di. I no get knock it.\n",
            "Monica: Oh, I'll-I'm gon, Chandler? I goinna later) And! A rem. Im not doestan a hhallway.\n",
            "Rachel: As! They sno?\n",
            "Monica: (entersing) Oh, my Weren-what?! (Ross gets jof you get hose baby something.)\n",
            "Joey: The hall before clap!\n",
            "Chandler: No, its-orsk, and post sheslemolating the trame?\n",
            "Joey: Frawlo it?\n",
            "Joey: Yknow, ' think!\n",
            "Monica: Oh-ellm-his?\n",
            "Chandler: Go no. The doles. Babolly to be you messelfr. And you know why ruld I! Wait look accoffeed, freed you screebreation eirs.)\n",
            "Monica: Ooties you gone me! Was it demesposs of a roommanotic!\n",
            "Monica: (in we any foot! (passs) She funmsely. Addrectically cool, whole go even.\n",
            "plaring: (pause) I know who so siltwer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 64 # how many independent sequences will we process in parallel?\n",
        "block_size = 256 # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "learning_rate = 3e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 384\n",
        "n_head = 6\n",
        "n_layer = 6\n",
        "dropout = 0.2\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "with open('/content/drive/MyDrive/Final_transcript.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # input of size (batch, time-step, channels)\n",
        "        # output of size (batch, time-step, head size)\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,hs)\n",
        "        q = self.query(x) # (B,T,hs)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,hs)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class GPTLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "        # better init, not covered in the original GPT video, but important, will cover in followup video\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = GPTLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))\n",
        "#open('more.txt', 'w').write(decode(m.generate(context, max_new_tokens=10000)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_OZ30Ava4Kj6",
        "outputId": "1279bbcc-0cba-4ff9-8e8a-8364283ff2ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10.810461 M parameters\n",
            "step 0: train loss 4.7288, val loss 4.7270\n",
            "step 500: train loss 1.4937, val loss 1.5426\n",
            "step 1000: train loss 1.2454, val loss 1.3124\n",
            "step 1500: train loss 1.1558, val loss 1.2364\n",
            "step 2000: train loss 1.1012, val loss 1.1856\n",
            "step 2500: train loss 1.0601, val loss 1.1497\n",
            "step 3000: train loss 1.0313, val loss 1.1346\n",
            "step 3500: train loss 1.0062, val loss 1.1152\n",
            "step 4000: train loss 0.9876, val loss 1.1037\n",
            "step 4500: train loss 0.9706, val loss 1.0971\n",
            "step 4999: train loss 0.9559, val loss 1.0862\n",
            "\n",
            "Rachel: Why is step with you girls to be tomorrow?\n",
            "Ross: You pick the duck to play you?\n",
            "Rachel: Because I suppose probably when show my because youre no noment delicing me.\n",
            "Ross: Because your best...\n",
            "Rachel: Credits. If you shouldnt flip me around her steppen on the duck and Joey jumps uh, giving my one of the chairs darks getting you closely on, the door theres left maturn.\n",
            "Rachel: Oh wow. I've-I've seen somebody rabbit!\n",
            "Ross: I'm sure what you-you're doing wiping!\n",
            "Chandler: Fine, just tell led\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(m.generate(context, max_new_tokens=1000)[0].tolist()))"
      ],
      "metadata": {
        "id": "qVP5gz4i8CaI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "318ecc44-267d-443f-c370-f3ea7827452d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Rachel: I know. (To Monica) Yeah, listen, thats whats on an elvet. Okay. Hey Ben, youre weat good done for who last night. We cant do the cobies to high dryd. Dont really talk like it looks like you. Because.\n",
            "Chandler: What? What are you thinking?\n",
            "Rachel: Chandler, its not a third thing thing thag we go out ago]\n",
            "Chandler: Okay, well you can uh\n",
            "Rachel: I know, kinda stay theaters, but hell sit was like Su Stins, do you give me. Absoluty?\n",
            "Rachel: (nerving his watch) Okay, Alan and a landr alan, Im hoping for me.\n",
            "(They rest and Atkes around the parents back, Gingers hanging again. Monica turns to her wait from before the star, Chandler trips back to resume Joey's (To Gingers) Hey! Phoebe is her in where ....\n",
            "Monica: I wish apparent!\n",
            "Chandler: Okay, thats tween you back, were getting a blair and.\n",
            "Joey: All right, boy, wait, I saw you to work. I mean our water old lunch.\n",
            "Chandler: Okay. (Monica sforthway again, Joey starts to take place between Chandler.) You are really sweating Phoebe. (Mo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(m.generate(context, max_new_tokens=10000)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9CsoF0J21yo2",
        "outputId": "0c5ae804-77ee-4af6-be34-47cb94e69020"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Rachel: Okay. No for you.\n",
            "Ross: NoOO.\n",
            "Mrs. Leedbetter: (embarraes at him) (over to him) Its like all this morning ignore.\n",
            "Ross: What? Maybe Julie?\n",
            "Emily: Yes, Rach, Im sorry Im sorry, and ahhh, listen before is (Starts to slamp him again) my trainess scredimaian and you should be going down for my repersonal night stupid for spin  arounced. (He returns and then goes cant fall notes, he goes back into the same way to wrestly sqhing.)\n",
            "Ross: Because Chandler makes me a friend.\n",
            "Joey: Would you ttell you that?\n",
            "(Theyre entering)\n",
            "Joey: Thats all right Joey.\n",
            "Ross: (entering, with Monica) Well, what do you think, what do you think?\n",
            "Joey: (with her in love) What do you saw? The same saucer? \n",
            "Ross: I dont think well, actually it first (laughs.) Oh, yeah, really. I do. I dont. You should miss my mom if-inational and I thought shes like pyace this. Why wouldnt I, work guy?\n",
            "Joey: Yeah.\n",
            "Ross: What?\n",
            "Janine: You suck up the lesbian cut? (High starts sigh) \n",
            "Monica: Well I dont think it is way you kinda cuse marry that uh, hes very even gonna come.\n",
            "Joey: Yeah, first now, like still, like for someone. Would you really say this years and nothing. Says youre agrees another little old? Shhhh! I didntly told ush this is the fact simes!\n",
            "Rachel: Yeah, es ummm, he still got breasts with stuff and bending coming! (Pause) Susan this is over comming in in inside.\n",
            "Paul: Good stupid. It's a girl. Y'kza? So ya well. Well, nevery should be a restaurant, and fered awes up like a girls baby.\n",
            "Rachel: (trying not to start cradling her suite potens to herself) Well, after ficast four restaurant girl is an accent, like at like drtig on the cover.\n",
            "Joey: Pheebs!!\n",
            "Phoebe: Thats pretty short.\n",
            "Rachel: (to Joey) Look, if come oursel! Were in in telling, you some bagel?\n",
            "Joey: Yeah umm, Im here. You just gave me a like tough. Youre in the moment, youre staring on the morning.\n",
            "Rachel: What ah, look were you not joinr a moment tonight. I really need maneth too.\n",
            "Joey: Really? Who? Did I?\n",
            "Joey: Well, Morning. Didnt you call yourselves up?\n",
            "Rachel: Oh you do.\n",
            "Joey: You gotta get the musicoversTag. Dont you insecome to tell me?\n",
            "Rachel: Ohh, huh. (starts to rembling, and he tried to feel the momens and hurtles)\n",
            "Joey: Hey, well Ross if the photographers he is, isnt he reclived in him?\n",
            "Joey: Oh, yeah, hes on the ride of the tape.\n",
            "[Scene: Phoebe and Janice]\n",
            "Janice: You must be a question, and Joey is the door.]\n",
            "Janice: Wow, is it Rom-Angela clusy, you have to turn Emma into it? Thats ready of backs! Thank you. (She is happy, Rissent is making Rachel and suddenly if he hearses I just like, \"Chandler, can I'll just make this good idea,\" calm.\n",
            "Monica: Chandler?\n",
            "Chandler: Wait! Wait, that's a muccdentunot, put my first int, somebody wanted to the straightendable! Y'know how, last sign or arming, then he tell nude members ago nuts how you can find over your droppy?\n",
            "Monica: I know. In take your thee triple.\n",
            "Chandler: Y'know, I'm just gonna say another cup of time I don't figure that's price up, wouldn't you. (Ross creams the cup out of the bandclounters. And to the face woman's gonna have a kiss or any to excellected, oooooooodby. Doofy-`Stuff area not awake, hey, I live behindow again... Oh, Ross, don't worrize and kisser his unamerance circulares will be now...hey, hey, hey, ere..\n",
            "(A lectures eventers.)\n",
            "ROSS: Ho, la-lace! Hello? Im sorry, she's a diaped of dinosaurs! \n",
            "[Scene: Phoebe's, Ross and Rachel, and Phoebe are there. Monica are enters]\n",
            "Monica: Hey! Wow, coma seen-maja, Statish!!!\n",
            "Ross: Where?\n",
            "Ross: How do you need some people? You don't get some recisted the dinosaus thing.\n",
            "Monica: What?\n",
            "Ross: Y'know just I mean, what do you things for all?\n",
            "Monica and Ross: Oh, we're not going, hopele's getting asking out her! There's nothing ready this, culp because ity luck!\n",
            "Chandler: (entering) Joey's working out!\n",
            "Joey: Little Magazine will do something!!\n",
            "Chandler: Pheebs!\n",
            "[Scene: Joey's Apartment, he hears Chandler and Monica are watching about the tapkin and I watch him about his waken.]\n",
            "End\n",
            "The Livious Went On TV: Their God New Sep, Carol Godie hims Help. \n",
            "Chandler: What are you doing hit for a pRachel? \n",
            "Monica: This is Pretty pleasure. Oh, Thanksgivin' good. You uh, I thought you are Uh-huh Wha. Whew, wh'm Im Joey Ben's? \n",
            "Monica: New York Huh, please, if you heat yours... \n",
            "Phoebe: Aww. \n",
            "(They all the two and lamp on the cereal Christmons sfit, Ross cereating alive.]\n",
            "Alice: New Yors? Whoa! \n",
            "<Tack down pictures coack again as Monica finality cilly] \n",
            "Ross: You're Ross?\n",
            "Monica and Ross: Relived? \n",
            "Ross: So, no. \n",
            "Monica: You feel sir. Okay, I slept with her folight with to you. (Chandler steps then he likes a parties of shirt botill, locking possides Joey.) \n",
            "[Scene: Central Perk]\n",
            "Danny: Well, I mean, you might I should call them like none.\n",
            "(Ross woman starts to find her place at Chandler has vircudent to the woman outside.)\n",
            "Ross: (ppause) Really? Im packing sposil. Im having a place. Im the one hand of your bus. Phoebe the star.? (Pause) That was Ross, come on sadwiches! (\n",
            "She they hug.) Paul do we?\n",
            "Ross: No. Woo, I just tell shes because were showin you.\n",
            "Joey: What?!!!\n",
            "[The hell happens: (Quit two glar again and points to Chandler.]\n",
            "Opening Credits\n",
            "[Scene: In the locu bedroom, starts out of the dinner]\n",
            "Chandler: Hey guys, this way I am just gone fight, I rememember my thing is a wouse, guy and eat this, and I get scared.(Joey starts crying again.) Fleasurket 2081? (He embraces her cushro with once.) I remind that figured you. And it's immediate if you and now you'd you're be your scool and I'm in full. Hey, yeah, so let me it just do that. (He keeps the budden trouble.) What're you doing? \n",
            "Chandler: Look honey, look, here you go. Honey, you guys wanna wanna buy it? \n",
            "(Ross checks up as if she got the cheers) \n",
            "Monica: (to Chandler) All right, he left embarry in a lone. \n",
            "Chandler: Went drawled onto her,   write  lip, you? \n",
            "Rachel: Wait-wait, umm, sorry I didn't tast a betray at the people. \n",
            "die: You too go Toesday.\n",
            "Rachel: (Leaving) You guys could please two girls for sauce last night! \n",
            "Joey: Noooo! (Shows her exciter.) All the waition.\n",
            "Rachel: Well, my last people ends wouldn't happ in communuments where it's leaving. .. They're not way enough another, he's just affected.\n",
            "Rachel: What God?\n",
            "Joey: Phoebe, so I don't want to be common with your rules and, do it, didn't you?\n",
            "Rachel: Organ isn't if you can't be married if you can stay astart. 'You're just a mint fater,    and you're way it, you're when I'm good. A'm I'm, when we done?\n",
            "Joey: You says he wanted a lot of gold.\n",
            "Rachel: Well is really sweet it? (He looks and looks at starts. A mach, who he does uh, Open the part is for the door, Joey, is that the fridge starts Joeys Geller is trying not trying on the floor.) Is that a floor cabble?! (Stops and looks up, at Joey, notices it's gonna be a huge date) Do you have to do that?\n",
            "Joey: Uhh, gave me can't get everydate acceptaritment?\n",
            "Joey: Yeah. (Holding each to get nonming ready.) Uh-oh!\n",
            "End\n",
            "THE ONE THE KINW BITH THE RICH ING\n",
            "[Scene: The Restation, Time lapting are still marriving admitting their hands on the garbages.]\n",
            "Kiston: Hey Joe. Well of course what its bheeping are marking that, so I say.\n",
            "Rachel: Oh-you are so liky!\n",
            "Kist: Im just saying what Im just a $500 four.\n",
            "Rachel: Yeah. Wow! Hey Just put here together! Theres the one play in the desk! (They all all walk enter, whisper.)\n",
            "Ross: (On phone) Think youre righ, Phoebe calme to pack the drop... (they kiss, Phoebe) OKay-Phil Frank, left the door, you put away from not her tonight. You get fired, vick completely...\n",
            "Phoebe: Thanks. Hey, hey. (They go to rush and fish the mouth children.) Hi!\n",
            "Ey/Kyley: Hey-hey Ross, it was a big girl from TV ange.\n",
            "Phoebe: (grabbing the bite) Okay, okay, Im just gonna show Dr. Hellos.\n",
            "Heley: (on machine voice) Hey, you look, daddawill you put somebody!\n",
            "Phoebe: Do it!\n",
            "[Scene: Pete's tea, Chandler and Monica hugs Ross.]\n",
            "Joey: (Triumpping the balcony putte and pretends up what he works is quite something.) You remember how to give me dinner awoment! (He notices that inced jamsas interest itding for a T(He walks.)\n",
            "Ross: Her in me!\n",
            "Joey: Im sorry! Im sorry, what its going on? (He laughs to Phoebe.)\n",
            "[Scene: Phoebe and Ross's erm, Phoebe is getting pretty mans very dicting, then... Joey: Hey nice. So, what is they aside?\n",
            "Phoebe: Come on, Joey, did imp? Hi. Oh, pis me out!\n",
            "Joey: Hang out.\n",
            "Phoebe: What?\n",
            "Joey: Ojust on i'a... Why?\n",
            "Phoebe: Hi, well, I was thinking, so uh... 'you kumm, y'know whichs... Stupid because I'm officiant a melt which you said to leav over, this is staigng, I  was just have to kid...  Well then we'd be bringinged puzzlens.\n",
            "Phoebe: Alright, fine, first week from at this Super Three'll...andlone goe it. It's an extrements. And the out weeks have to be metine. (Rachel glares at him.) Oh, oh, I'm sorry, up.. (Phoebe glares at him)\n",
            "Phoebe: Oh, so that's fame to come a dry.\n",
            "Rachel, is serious last nanukes...\n",
            "(Phoebe's enter. WeesigH shand...ampled to Monica who's fire there in.)\n",
            "Chandler: Phoebe, yeah, that- I would rap.\n",
            "Phoebe: Rach, Phoebe... ohh...jo... I uh, I wasn't here and had something finishes potariture who else, y'know, the-\"Croge! And geting bacsee from The One?\n",
            "Monica: Oh Chandler!\n",
            "Phoebe: Well, thats okay, it's a larm.\n",
            "Chandler: (enters vegetara to Chandler) Hey, it's so difficul, it'll be hot so you good... a little.\n",
            "(Director gets up for Joey, Joey is facing an empty brediculouse for each other door. But she smells her and straigh. After the door leaves, the second you could do invited something expression on that before Joey agrocues. After a woman, then the left the trip is unlocked, dressed you around to wait under to place! Weve excering the best pulling to ours in personal, they don't think isn't procus.\n",
            "Joey: Look, this is why you can teach peeks.\n",
            "Ross: (shot her suitcases up and mines the bagely back) for Joey's!\n",
            "Kath: Yes.. exactly it's all right. I mean, I guess that is with right now.\n",
            "Joey: Yeah.\n",
            "Ross: (sets the door and stops bedoor) Oh no-no-no-no-no, no to go. (\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "paESmt0_2AmP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}